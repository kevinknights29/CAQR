{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1"
      ],
      "metadata": {
        "id": "uxikFeR7SiSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Matlab or Python, implement a version of the TSQR that divides an input matrix up into four blocks of rows (using row sub-indexing) and computes the QR-factorisation in the way shown in the lectures on communication-avoiding factorisations."
      ],
      "metadata": {
        "id": "yNVSydOPNkoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TSQR defines a family of algorithms, in which the QR factorization of A is obtained by performing a sequence of QR factorizations until the lower trapezoidal part of A is annihilated and the final R factor is obtained. The QR factorizations are performed on block rows of A and on previously obtained R factors, stacked atop one another. We call the pattern followed during this sequence of QR factorizations a reduction tree."
      ],
      "metadata": {
        "id": "2gOtkRy5NyWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequential TSQR\n",
        "\n",
        "The first set of algorithms, “Tall Skinny QR” (TSQR), are for matrices for which the number of rows is much larger than the number of columns, and which have their rows dis-\n",
        "tributed over processors in a one-dimensional (1-D) block row layout.\n",
        "\n",
        "Sequential TSQR uses a similar factorization process, but with a “flat tree” (a linear chain). We start with the same block row decomposition as with parallel TSQR,\n",
        "but begin with a QR factorization of $A_0$, rather than of all the block rows:\n",
        "\n",
        "$$\n",
        "A =\n",
        "\\begin{pmatrix}\n",
        "A_0 \\\\\n",
        "A_1 \\\\\n",
        "A_2 \\\\\n",
        "A_3\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "Q_{00} R_{00} \\\\\n",
        "A_1 \\\\\n",
        "A_2 \\\\\n",
        "A_3\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "This is “stage 0” of the computation, hence the second subscript 0 of the $Q$ and $R$ factor. We then combine $R_{00}$ and $A_1$ using a QR factorization:\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "R_{00} \\\\\n",
        "A_1 \\\\\n",
        "A_2 \\\\\n",
        "A_3\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "R_{00} \\\\\n",
        "A_1 \\\\\n",
        "\\hline\n",
        "A_2 \\\\\n",
        "A_3\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "Q_{01} R_{01} \\\\\n",
        "\\hline\n",
        "A_2 \\\\\n",
        "A_3\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "We continue this process until we run out of $A_i$ factors. Here, the $A_i$ blocks are $m/P \\times n$. If we were to compute all the above $Q$ factors explicitly as square matrices, which we do not, then $Q_{00}$ would be $m/P \\times m/P$ and $Q_{0j}$ for $j > 0$ would be $2m/P \\times 2m/P$ . The final R factor, as in the parallel case, would be $m \\times n$ upper triangular (or $n \\times n$ upper triangular in a “thin QR”)."
      ],
      "metadata": {
        "id": "p9zVwE2BSbeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Sequential implementation of TSQR\n",
        "p = 4                                       # 4 block rows\n",
        "m = 4 * p                                   # So each block has 4 rows\n",
        "n = 5                                       # So each block has 5 columns\n",
        "scaling_factor = 100                        # Scales elements, otherwise [0, 1)\n",
        "A = scaling_factor * np.random.rand(m, n)   # Matrix\n",
        "is_thin = True                              # Outputs a Thin-QR (n x n) matrix\n",
        "print(f\"A ({m} x {n}):\\n{A}\\n\")\n",
        "\n",
        "# Partioning A into p blocks\n",
        "m = A.shape[0] // p\n",
        "A_partitioned = [None] * p\n",
        "for i in range(p):\n",
        "  A_partitioned[i] = A[m * i: m * (i + 1), :]\n",
        "  print(f\"A_{i} ({m} x {n}):\")\n",
        "  print(f\"{A_partitioned[i]}\\n\")\n",
        "\n",
        "R = None\n",
        "for i in range(p):\n",
        "  if i == 0:\n",
        "    # QR factorization of A_0\n",
        "    _, R = np.linalg.qr(A_partitioned[i], \"reduced\" if is_thin else \"complete\")\n",
        "    print(f\"R_0{i} ({R.shape[0]} x {R.shape[1]}):\\n{R}\\n\")\n",
        "  else:\n",
        "    # Combine R_i-1 and A_i using a QR factorization\n",
        "    stacked_matrix = np.vstack((R, A_partitioned[i]))\n",
        "    _, R = np.linalg.qr(stacked_matrix, \"reduced\" if is_thin else \"complete\")\n",
        "    print(f\"R_0{i} ({R.shape[0]} x {R.shape[1]}):\\n{R}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QalCOhz4YkXp",
        "outputId": "d03207cd-01ce-48e9-918f-2cc75c35692e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A (16 x 5):\n",
            "[[58.53349999 88.42089695 12.82532575 40.61157464 22.40800689]\n",
            " [24.51354352 16.53395331 96.39468142 13.71181688  3.50752132]\n",
            " [33.86832371 25.73668137 80.44033633 18.08295237 92.24711753]\n",
            " [20.64349387 69.69353981  2.916204   58.665973   93.01198732]\n",
            " [40.31163915 11.93834246 72.24052454 93.64964444 81.36784117]\n",
            " [ 5.06878922 44.90567116 34.32396244 77.78521312 38.66211803]\n",
            " [19.89933233 76.31266345 29.97953635 35.53252812 59.48891768]\n",
            " [34.28306129 34.34437439 89.78893969 26.70895617 26.17772715]\n",
            " [ 1.5479483  56.00301572  2.1451451  94.6896275  73.1563357 ]\n",
            " [96.95471517  7.94907908 66.78736314 13.3974737  78.25693216]\n",
            " [60.21906999 21.39828089 94.12489267 94.51912604 33.93554974]\n",
            " [61.68342475 75.06972763  2.14119113 41.80249993 51.96563237]\n",
            " [52.46807673 17.62938226 20.74956398 44.28847085 52.4703498 ]\n",
            " [19.0809091  25.28585502 49.1675797  96.02725829 12.31429051]\n",
            " [55.30151763 39.3868871  79.93471707  0.45232731 46.53816409]\n",
            " [52.44749872 20.22446177 48.75975109 79.63918909 46.54767331]]\n",
            "\n",
            "A_0 (4 x 5):\n",
            "[[58.53349999 88.42089695 12.82532575 40.61157464 22.40800689]\n",
            " [24.51354352 16.53395331 96.39468142 13.71181688  3.50752132]\n",
            " [33.86832371 25.73668137 80.44033633 18.08295237 92.24711753]\n",
            " [20.64349387 69.69353981  2.916204   58.665973   93.01198732]]\n",
            "\n",
            "A_1 (4 x 5):\n",
            "[[40.31163915 11.93834246 72.24052454 93.64964444 81.36784117]\n",
            " [ 5.06878922 44.90567116 34.32396244 77.78521312 38.66211803]\n",
            " [19.89933233 76.31266345 29.97953635 35.53252812 59.48891768]\n",
            " [34.28306129 34.34437439 89.78893969 26.70895617 26.17772715]]\n",
            "\n",
            "A_2 (4 x 5):\n",
            "[[ 1.5479483  56.00301572  2.1451451  94.6896275  73.1563357 ]\n",
            " [96.95471517  7.94907908 66.78736314 13.3974737  78.25693216]\n",
            " [60.21906999 21.39828089 94.12489267 94.51912604 33.93554974]\n",
            " [61.68342475 75.06972763  2.14119113 41.80249993 51.96563237]]\n",
            "\n",
            "A_3 (4 x 5):\n",
            "[[52.46807673 17.62938226 20.74956398 44.28847085 52.4703498 ]\n",
            " [19.0809091  25.28585502 49.1675797  96.02725829 12.31429051]\n",
            " [55.30151763 39.3868871  79.93471707  0.45232731 46.53816409]\n",
            " [52.44749872 20.22446177 48.75975109 79.63918909 46.54767331]]\n",
            "\n",
            "R_00 (4 x 5):\n",
            "[[ -74.83516304 -105.44864809  -78.81677774  -60.6235435   -86.08183925]\n",
            " [   0.           49.91768729  -66.30521136   39.64498972   36.43167579]\n",
            " [   0.            0.          -72.98705388  -18.06896715  -54.09239089]\n",
            " [   0.            0.            0.           -5.70759673  -77.53608936]]\n",
            "\n",
            "R_01 (5 x 5):\n",
            "[[ 93.92728156 120.26473526 134.77653744 109.96760984 127.75023211]\n",
            " [  0.         -91.15804034  37.65119136 -37.14795081 -40.35092947]\n",
            " [  0.           0.         108.21692271  27.2093144   36.57161146]\n",
            " [  0.           0.           0.          90.49605183  55.03896795]\n",
            " [  0.           0.           0.           0.          87.74839917]]\n",
            "\n",
            "R_02 (5 x 5):\n",
            "[[-160.17530533 -112.83057993 -155.69224965 -125.14324059 -155.75957968]\n",
            " [   0.          139.05226053  -14.12640746   93.93322187   77.76910644]\n",
            " [   0.            0.         -142.06629181  -58.38132485  -36.55429244]\n",
            " [   0.            0.            0.         -119.73721128  -49.66905142]\n",
            " [   0.            0.            0.            0.         -105.97455098]]\n",
            "\n",
            "R_03 (5 x 5):\n",
            "[[ 185.96264835  122.16973     182.52452258  152.73371267  177.19560816]\n",
            " [   0.         -141.60280121    6.72916086  -94.34530893  -75.92552076]\n",
            " [   0.            0.          151.25160471   57.9687524    27.75067417]\n",
            " [   0.            0.            0.          155.45406769   40.38144438]\n",
            " [   0.            0.            0.            0.          114.08967413]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential implementation of TSQR\n",
        "p = 4                                       # 4 block rows\n",
        "m = 4 * p                                   # So each block has 4 rows\n",
        "n = 5                                       # So each block has 5 columns\n",
        "scaling_factor = 100                        # Scales elements, otherwise [0, 1)\n",
        "A = scaling_factor * np.random.rand(m, n)   # Matrix\n",
        "is_thin = False                             # Outputs a Full-QR (m x n) matrix\n",
        "print(f\"A ({m} x {n}):\\n{A}\\n\")\n",
        "\n",
        "# Partitioning A into p blocks\n",
        "m = A.shape[0] // p\n",
        "A_partitioned = [None] * p\n",
        "for i in range(p):\n",
        "  A_partitioned[i] = A[m * i: m * (i + 1), :]\n",
        "  print(f\"A_{i} ({m} x {n}):\")\n",
        "  print(f\"{A_partitioned[i]}\\n\")\n",
        "\n",
        "R = None\n",
        "for i in range(p):\n",
        "  if i == 0:\n",
        "    # QR factorization of A_0\n",
        "    _, R = np.linalg.qr(A_partitioned[i], \"reduced\" if is_thin else \"complete\")\n",
        "    print(f\"R_0{i} ({R.shape[0]} x {R.shape[1]}):\\n{R}\\n\")\n",
        "  else:\n",
        "    # Combine R_i-1 and A_i using a QR factorization\n",
        "    stacked_matrix = np.vstack((R, A_partitioned[i]))\n",
        "    _, R = np.linalg.qr(stacked_matrix, \"reduced\" if is_thin else \"complete\")\n",
        "    print(f\"R_0{i} ({R.shape[0]} x {R.shape[1]}):\\n{R}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFakfyWXD5Q7",
        "outputId": "bc15c369-e770-4517-e84a-bc7211e6a8a5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A (16 x 5):\n",
            "[[21.23174828 76.04702134 11.85218571 64.90812265 95.47436312]\n",
            " [59.60568762 68.24819413 97.77735861 26.82616224 89.12871102]\n",
            " [96.4329713  23.04488324 83.75266762 95.18804405  2.40524975]\n",
            " [10.24285472  0.55982992 39.53503927 20.60335248 31.15579189]\n",
            " [50.48722892 15.93282684 58.18978182 91.43790113 25.17005484]\n",
            " [63.8051006  37.1902777  85.0024035  93.65295314  1.64699034]\n",
            " [23.7217667  97.53014673 90.0279199  97.48250487 51.43918441]\n",
            " [83.2493308  35.62143565 97.27957507 61.37533496  3.79768252]\n",
            " [ 1.22800186 80.31995598 44.7729469  91.11571356 24.10982419]\n",
            " [35.11723308 22.53474916 24.38285548 60.62742648 83.89361636]\n",
            " [87.62622085 42.02921515 73.38342817 16.70500542 85.95225935]\n",
            " [63.59083231 86.03202152 47.16903553  3.51176619 59.99676095]\n",
            " [36.51423056 18.98038157 64.11500389 25.3078171  87.4257058 ]\n",
            " [84.37137557 37.75816316 35.30713594 74.75826349 14.26655214]\n",
            " [52.02310234 98.10528845 14.5224535  73.79725474 63.7235424 ]\n",
            " [36.91796856 12.51193721  8.10179467 69.58536972 67.30243572]]\n",
            "\n",
            "A_0 (4 x 5):\n",
            "[[21.23174828 76.04702134 11.85218571 64.90812265 95.47436312]\n",
            " [59.60568762 68.24819413 97.77735861 26.82616224 89.12871102]\n",
            " [96.4329713  23.04488324 83.75266762 95.18804405  2.40524975]\n",
            " [10.24285472  0.55982992 39.53503927 20.60335248 31.15579189]]\n",
            "\n",
            "A_1 (4 x 5):\n",
            "[[50.48722892 15.93282684 58.18978182 91.43790113 25.17005484]\n",
            " [63.8051006  37.1902777  85.0024035  93.65295314  1.64699034]\n",
            " [23.7217667  97.53014673 90.0279199  97.48250487 51.43918441]\n",
            " [83.2493308  35.62143565 97.27957507 61.37533496  3.79768252]]\n",
            "\n",
            "A_2 (4 x 5):\n",
            "[[ 1.22800186 80.31995598 44.7729469  91.11571356 24.10982419]\n",
            " [35.11723308 22.53474916 24.38285548 60.62742648 83.89361636]\n",
            " [87.62622085 42.02921515 73.38342817 16.70500542 85.95225935]\n",
            " [63.59083231 86.03202152 47.16903553  3.51176619 59.99676095]]\n",
            "\n",
            "A_3 (4 x 5):\n",
            "[[36.51423056 18.98038157 64.11500389 25.3078171  87.4257058 ]\n",
            " [84.37137557 37.75816316 35.30713594 74.75826349 14.26655214]\n",
            " [52.02310234 98.10528845 14.5224535  73.79725474 63.7235424 ]\n",
            " [36.91796856 12.51193721  8.10179467 69.58536972 67.30243572]]\n",
            "\n",
            "R_00 (4 x 5):\n",
            "[[-115.79231045  -68.31725306 -125.75273365 -106.8068146   -68.1455906 ]\n",
            " [   0.          -79.40465001  -11.78235264  -21.09813555 -110.33063663]\n",
            " [   0.            0.          -48.22697349   31.11812952  -29.23867053]\n",
            " [   0.            0.            0.           39.96003348   19.08347409]]\n",
            "\n",
            "R_01 (8 x 5):\n",
            "[[165.89479678  98.65887686 199.8658716  183.13596066  65.11940794]\n",
            " [  0.         116.86213866  52.55917477  64.50039571 107.8716332 ]\n",
            " [  0.           0.          61.23220343   3.50431086 -13.06848343]\n",
            " [  0.           0.           0.         -84.30980839  19.8733406 ]\n",
            " [  0.           0.           0.           0.         -69.92742376]\n",
            " [  0.           0.           0.           0.           0.        ]\n",
            " [  0.           0.           0.           0.           0.        ]\n",
            " [  0.           0.           0.           0.           0.        ]]\n",
            "\n",
            "R_02 (12 x 5):\n",
            "[[-201.19135215 -131.27151978 -216.20103489 -170.531054   -124.88412097]\n",
            " [   0.         -149.28226764  -58.73168812  -86.47127697 -102.07615103]\n",
            " [   0.            0.          -80.46802721  -64.73383743   35.39426959]\n",
            " [   0.            0.            0.          127.82824844  -13.61474318]\n",
            " [   0.            0.            0.            0.          111.73867249]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]]\n",
            "\n",
            "R_03 (16 x 5):\n",
            "[[ 230.21537266  155.74595697  216.63368747  208.27904934  153.42738121]\n",
            " [   0.          163.7917403    45.6981256    87.11986658  103.9609551 ]\n",
            " [   0.            0.          115.25101802   34.29661743   -2.26968113]\n",
            " [   0.            0.            0.         -146.49164982   14.86929267]\n",
            " [   0.            0.            0.            0.         -147.65580993]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.            0.            0.        ]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "- [1] Jammes Demmel, Laura Grigori, Mark Hoemmen, and Julien Langou. Implementing Communication-Optimal Parallel And Sequential QR Factorizations. https://arxiv.abs/pdf/0809.2407, 2008. arXiv:0809.2407 [math.NA]"
      ],
      "metadata": {
        "id": "Ol13ebYfUaWA"
      }
    }
  ]
}