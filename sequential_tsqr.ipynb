{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1"
      ],
      "metadata": {
        "id": "uxikFeR7SiSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Matlab or Python, implement a version of the TSQR that divides an input matrix up into four blocks of rows (using row sub-indexing) and computes the QR-factorisation in the way shown in the lectures on communication-avoiding factorisations."
      ],
      "metadata": {
        "id": "yNVSydOPNkoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TSQR defines a family of algorithms, in which the QR factorization of A is obtained by performing a sequence of QR factorizations until the lower trapezoidal part of A is annihilated and the final R factor is obtained. The QR factorizations are performed on block rows of A and on previously obtained R factors, stacked atop one another. We call the pattern followed during this sequence of QR factorizations a reduction tree."
      ],
      "metadata": {
        "id": "2gOtkRy5NyWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequential TSQR\n",
        "\n",
        "Sequential TSQR uses a similar factorization process, but with a “flat tree” (a linear chain). We start with the same block row decomposition as with parallel TSQR,\n",
        "but begin with a QR factorization of $A_0$, rather than of all the block rows:\n",
        "\n",
        "$$\n",
        "A =\n",
        "\\begin{pmatrix}\n",
        "A_0 \\\\\n",
        "A_1 \\\\\n",
        "A_2 \\\\\n",
        "A_3\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "Q_{00} R_{00} \\\\\n",
        "A_1 \\\\\n",
        "A_2 \\\\\n",
        "A_3\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "This is “stage 0” of the computation, hence the second subscript 0 of the $Q$ and $R$ factor. We then combine $R_{00}$ and $A_1$ using a QR factorization:\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "R_{00} \\\\\n",
        "A_1 \\\\\n",
        "A_2 \\\\\n",
        "A_3\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "R_{00} \\\\\n",
        "A_1 \\\\\n",
        "\\hline\n",
        "A_2 \\\\\n",
        "A_3\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "Q_{01} R_{01} \\\\\n",
        "\\hline\n",
        "A_2 \\\\\n",
        "A_3\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "We continue this process until we run out of $A_i$ factors. Here, the $A_i$ blocks are $m/P \\times n$. If we were to compute all the above $Q$ factors explicitly as square matrices, which we do not, then $Q_{00}$ would be $m/P \\times m/P$ and $Q_{0j}$ for $j > 0$ would be $2m/P \\times 2m/P$ . The final R factor, as in the parallel case, would be $m \\times n$ upper triangular (or $n \\times n$ upper triangular in a “thin QR”)."
      ],
      "metadata": {
        "id": "p9zVwE2BSbeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reusing code from Numerical Methods (MAP55631) Assignment 3\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def mgs(A: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "  \"\"\"\n",
        "  Computes the Modified Grahm-Schmidt Algorithm.\n",
        "\n",
        "  Args:\n",
        "    A: an input matrix\n",
        "\n",
        "  Returns:\n",
        "    Tuple[np.ndarray, np.ndarray]: matrixes Q, R\n",
        "      as the result of the factorization algorithm.\n",
        "  \"\"\"\n",
        "  # Get shape from input matrix A\n",
        "  m, n = A.shape\n",
        "\n",
        "  # Create zeros matrix for Q with size m x n (same as A)\n",
        "  # Q contains the orthonormal vectors that span the columns space of A\n",
        "  Q = np.zeros((m, n))\n",
        "\n",
        "  # Create zeros matrix R with size n x n.\n",
        "  # R is an upper triangular matrix\n",
        "  R = np.zeros((n, n))\n",
        "\n",
        "  # Initialize V as copy of A\n",
        "  V = A.copy()\n",
        "  V = V.astype(np.float64)\n",
        "\n",
        "  # Iterate over the column spaces of A\n",
        "  for i in range(n):\n",
        "    v_i = V[:, i]\n",
        "\n",
        "    # Compute and store the norm of the vector\n",
        "    r_ii = np.linalg.norm(v_i)\n",
        "    R[i, i] = r_ii\n",
        "\n",
        "    # Compute and store the orthonormal vector\n",
        "    q_i = v_i / r_ii\n",
        "    Q[:, i] = q_i\n",
        "\n",
        "    # Iterate over the subsequent column spaces of A\n",
        "    for j in range(i + 1, n):\n",
        "      v_j = V[:, j]\n",
        "\n",
        "      # Compute and store the values of R\n",
        "      r_ij = np.dot(q_i, v_j)\n",
        "      R[i, j] = r_ij\n",
        "\n",
        "      # Update and store the orthogonal vector\n",
        "      v_j = v_j - r_ij * q_i\n",
        "      V[:, j] = v_j\n",
        "\n",
        "  return Q, R"
      ],
      "metadata": {
        "id": "nY36cI6iW-fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential implementation of TSQR\n",
        "p = 4                                     # 4 block rows\n",
        "m = 4 * p                                 # So each block has 4 rows\n",
        "n = 5                                     # So each block has 5 columns\n",
        "scaling_factor = 100                      # Scales elements, otherwise [0, 1)\n",
        "A = scaling_factor * np.random.rand(m,n)  # Matrix\n",
        "print(f\"A ({m} x {n}):\\n{A}\\n\")\n",
        "\n",
        "# Partioning A into p blocks\n",
        "m = A.shape[0] // p\n",
        "A_partitioned = [None] * p\n",
        "for i in range(p):\n",
        "  A_partitioned[i] = A[m * i: m * (i + 1), :]\n",
        "  print(f\"A_{i} ({m} x {n}):\")\n",
        "  print(f\"{A_partitioned[i]}\\n\")\n",
        "\n",
        "R = None\n",
        "for i in range(p):\n",
        "  if i == 0:\n",
        "    # QR factorization of A_0\n",
        "    _, R = mgs(A_partitioned[i])\n",
        "    print(f\"R_0{i} ({R.shape[0]} x {R.shape[1]}):\\n{R}\\n\")\n",
        "  else:\n",
        "    # Combine R_i-1 and A_i using a QR factorization\n",
        "    _, R = mgs(np.vstack((R, A_partitioned[i])))\n",
        "    print(f\"R_0{i} ({R.shape[0]} x {R.shape[1]}):\\n{R}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QalCOhz4YkXp",
        "outputId": "87fb794b-4bb0-4054-806d-b12845ce830d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A (16 x 5):\n",
            "[[34.1572165  21.72268007 65.54051431 76.64354738 44.643707  ]\n",
            " [ 1.41145446 32.05911413 32.23835764 50.54438836  5.97530075]\n",
            " [94.24281117 85.22069052 56.27136413 87.20651082 92.00762581]\n",
            " [69.2376024  88.12161375 75.5770929  20.74396316 54.72859737]\n",
            " [ 9.7568212  36.6964823  73.80374974 64.55011863 76.89632771]\n",
            " [82.74091577 76.78461728 54.61029379 66.67638665 67.70609209]\n",
            " [85.16715277 61.08335699 47.07965789 67.61358576 72.02214555]\n",
            " [52.8480907  28.57517341 84.78079193 10.45826031 36.08808044]\n",
            " [61.82156914 95.24151836  4.59728    37.49612165  7.06004991]\n",
            " [81.57417923 88.4728492  90.07348106  4.33301899 99.70415142]\n",
            " [53.94841514 41.46845309 96.26794828 25.58718201 85.0031129 ]\n",
            " [61.30285759 28.83154644 44.66706791 43.15726049 91.66239379]\n",
            " [47.14410896 71.94691231 40.56471179 83.49636391 80.86870032]\n",
            " [84.33560381 91.71584817 82.29106198 48.65553016 10.2297816 ]\n",
            " [60.44621231 80.18153101 35.45623732 82.17832829 78.71792432]\n",
            " [97.26961153 51.87399115 46.34693454 38.19515184  9.90381152]]\n",
            "\n",
            "A_0 (4 x 5):\n",
            "[[34.1572165  21.72268007 65.54051431 76.64354738 44.643707  ]\n",
            " [ 1.41145446 32.05911413 32.23835764 50.54438836  5.97530075]\n",
            " [94.24281117 85.22069052 56.27136413 87.20651082 92.00762581]\n",
            " [69.2376024  88.12161375 75.5770929  20.74396316 54.72859737]]\n",
            "\n",
            "A_1 (4 x 5):\n",
            "[[ 9.7568212  36.6964823  73.80374974 64.55011863 76.89632771]\n",
            " [82.74091577 76.78461728 54.61029379 66.67638665 67.70609209]\n",
            " [85.16715277 61.08335699 47.07965789 67.61358576 72.02214555]\n",
            " [52.8480907  28.57517341 84.78079193 10.45826031 36.08808044]]\n",
            "\n",
            "A_2 (4 x 5):\n",
            "[[61.82156914 95.24151836  4.59728    37.49612165  7.06004991]\n",
            " [81.57417923 88.4728492  90.07348106  4.33301899 99.70415142]\n",
            " [53.94841514 41.46845309 96.26794828 25.58718201 85.0031129 ]\n",
            " [61.30285759 28.83154644 44.66706791 43.15726049 91.66239379]]\n",
            "\n",
            "A_3 (4 x 5):\n",
            "[[47.14410896 71.94691231 40.56471179 83.49636391 80.86870032]\n",
            " [84.33560381 91.71584817 82.29106198 48.65553016 10.2297816 ]\n",
            " [60.44621231 80.18153101 35.45623732 82.17832829 78.71792432]\n",
            " [97.26961153 51.87399115 46.34693454 38.19515184  9.90381152]]\n",
            "\n",
            "R_00 (5 x 5):\n",
            "[[ 1.21837025e+02  1.22458694e+02  1.05223619e+02  1.01316659e+02\n",
            "   1.14855667e+02]\n",
            " [ 0.00000000e+00  3.91345731e+01  2.62469121e+01  3.52620165e+00\n",
            "  -6.13282406e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  4.95209413e+01  4.79446832e+01\n",
            "   1.02515617e+01]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  6.23543415e+01\n",
            "   1.24441572e+01]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   1.59145736e-14]]\n",
            "\n",
            "R_01 (5 x 5):\n",
            "[[178.41490966 158.86403518 148.80411014 139.01273907 159.10726215]\n",
            " [  0.          55.49856896  44.56088237  42.85465448  36.03401965]\n",
            " [  0.           0.          88.98569103  33.79821061  33.41825986]\n",
            " [  0.           0.           0.          86.3383145   32.61751833]\n",
            " [  0.           0.           0.           0.          41.01015062]]\n",
            "\n",
            "R_02 (5 x 5):\n",
            "[[221.3065859  205.38659516 190.29020393 142.33434586 213.10619921]\n",
            " [  0.          74.72283384   9.58799947  19.92461249 -11.1299587 ]\n",
            " [  0.           0.         123.05092032  26.66051152  73.9570561 ]\n",
            " [  0.           0.           0.         111.05329696  21.18900463]\n",
            " [  0.           0.           0.           0.          68.10010321]]\n",
            "\n",
            "R_03 (5 x 5):\n",
            "[[267.25790531 248.72088782 215.58304311 180.43200457 215.36711218]\n",
            " [  0.          92.80743074  20.3920079   52.64114854  31.82024913]\n",
            " [  0.           0.         127.96263442  20.78586243  76.89513078]\n",
            " [  0.           0.           0.         124.46675713  51.41991578]\n",
            " [  0.           0.           0.           0.         114.37208715]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "- [1] Jammes Demmel, Laura Grigori, Mark Hoemmen, and Julien Langou. Implementing Communicaiton-Optimal Parallel And Sequential QR Factorizations. https://arxiv.abs/pdf/0809.2407, 2008. arXiv:0809.2407 [math.NA]"
      ],
      "metadata": {
        "id": "Ol13ebYfUaWA"
      }
    }
  ]
}