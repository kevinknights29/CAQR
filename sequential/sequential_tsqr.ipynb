{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1"
      ],
      "metadata": {
        "id": "uxikFeR7SiSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Matlab or Python, implement a version of the TSQR that divides an input matrix up into four blocks of rows (using row sub-indexing) and computes the QR-factorisation in the way shown in the lectures on communication-avoiding factorisations."
      ],
      "metadata": {
        "id": "yNVSydOPNkoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TSQR defines a family of algorithms, in which the QR factorization of A is obtained by performing a sequence of QR factorizations until the lower trapezoidal part of A is annihilated and the final R factor is obtained. The QR factorizations are performed on block rows of A and on previously obtained R factors, stacked atop one another. We call the pattern followed during this sequence of QR factorizations a reduction tree."
      ],
      "metadata": {
        "id": "2gOtkRy5NyWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequential TSQR\n",
        "\n",
        "The first set of algorithms, “Tall Skinny QR” (TSQR), are for matrices for which the number of rows is much larger than the number of columns, and which have their rows dis-\n",
        "tributed over processors in a one-dimensional (1-D) block row layout.\n",
        "\n",
        "Sequential TSQR uses a similar factorization process, but with a “flat tree” (a linear chain). We start with the same block row decomposition as with parallel TSQR,\n",
        "but begin with a QR factorization of $A_0$, rather than of all the block rows:\n",
        "\n",
        "$$\n",
        "A =\n",
        "\\begin{pmatrix}\n",
        "A_0 \\\\\n",
        "A_1 \\\\\n",
        "A_2 \\\\\n",
        "A_3\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "Q_{00} R_{00} \\\\\n",
        "A_1 \\\\\n",
        "A_2 \\\\\n",
        "A_3\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "This is “stage 0” of the computation, hence the second subscript 0 of the $Q$ and $R$ factor. We then combine $R_{00}$ and $A_1$ using a QR factorization:\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "R_{00} \\\\\n",
        "A_1 \\\\\n",
        "A_2 \\\\\n",
        "A_3\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "R_{00} \\\\\n",
        "A_1 \\\\\n",
        "\\hline\n",
        "A_2 \\\\\n",
        "A_3\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "Q_{01} R_{01} \\\\\n",
        "\\hline\n",
        "A_2 \\\\\n",
        "A_3\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "We continue this process until we run out of $A_i$ factors. Here, the $A_i$ blocks are $m/P \\times n$. If we were to compute all the above $Q$ factors explicitly as square matrices, which we do not, then $Q_{00}$ would be $m/P \\times m/P$ and $Q_{0j}$ for $j > 0$ would be $2m/P \\times 2m/P$ . The final R factor, as in the parallel case, would be $m \\times n$ upper triangular (or $n \\times n$ upper triangular in a “thin QR”)."
      ],
      "metadata": {
        "id": "p9zVwE2BSbeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Sequential implementation of TSQR\n",
        "# ============================================================\n",
        "p = 4                                       # 4 block rows\n",
        "m = 4 * p                                   # So each block has 4 rows\n",
        "n = 3                                       # So each block has 3 columns (m > n)\n",
        "scaling_factor = 100                        # Scales elements, otherwise [0, 1)\n",
        "np.random.seed(42)                          # Seeds random number generator\n",
        "A = scaling_factor * np.random.rand(m, n)   # Matrix\n",
        "print(f\"A ({m} x {n}):\\n{A}\\n\")\n",
        "\n",
        "# Partioning A into p blocks\n",
        "mb = A.shape[0] // p\n",
        "A_partitioned = [None] * p\n",
        "for i in range(p):\n",
        "  A_partitioned[i] = A[mb * i: mb * (i + 1), :]\n",
        "  print(f\"A_{i} ({mb} x {n}):\")\n",
        "  print(f\"{A_partitioned[i]}\\n\")\n",
        "\n",
        "# Sequential TSQR with FULL QR (mode='complete')\n",
        "# Full QR gives square orthogonal Q matrices, which we can embed into m x m\n",
        "Qs_full   = [None] * p\n",
        "R_current = None\n",
        "\n",
        "for i in range(p):\n",
        "    if i == 0:\n",
        "        Q_f, R_f   = np.linalg.qr(A_blocks[i], mode='complete')\n",
        "        Qs_full[i] = Q_f                                        # mb x mb  (square)\n",
        "        R_current  = R_f[:n, :]                                 # n x n (thin R, upper triangular)\n",
        "        print(f\"R_0{i} ({R_current.shape[0]} x {R_current.shape[1]}):\\n{R_current}\\n\")\n",
        "    else:\n",
        "        stacked    = np.vstack((R_current, A_blocks[i]))        # (n + mb) x n\n",
        "        Q_f, R_f   = np.linalg.qr(stacked, mode='complete')\n",
        "        Qs_full[i] = Q_f                                        # (n + mb) x (n + mb)  (square)\n",
        "        R_current  = R_f[:n, :]                                 # n x n\n",
        "        print(f\"R_0{i} ({R_current.shape[0]} x {R_current.shape[1]}):\\n{R_current}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QalCOhz4YkXp",
        "outputId": "4e99d877-9183-4297-bae1-44da7050246a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A (16 x 3):\n",
            "[[37.45401188 95.07143064 73.19939418]\n",
            " [59.86584842 15.60186404 15.59945203]\n",
            " [ 5.80836122 86.61761458 60.11150117]\n",
            " [70.80725778  2.05844943 96.99098522]\n",
            " [83.24426408 21.23391107 18.18249672]\n",
            " [18.34045099 30.4242243  52.47564316]\n",
            " [43.19450186 29.12291402 61.18528947]\n",
            " [13.94938607 29.21446485 36.63618433]\n",
            " [45.60699842 78.51759614 19.96737822]\n",
            " [51.42344384 59.24145689  4.64504127]\n",
            " [60.75448519 17.05241237  6.5051593 ]\n",
            " [94.88855373 96.56320331 80.83973481]\n",
            " [30.46137692  9.7672114  68.42330265]\n",
            " [44.01524937 12.20382348 49.51769101]\n",
            " [ 3.43885211 90.93204021 25.87799816]\n",
            " [66.25222844 31.17110761 52.00680212]]\n",
            "\n",
            "A_0 (4 x 3):\n",
            "[[37.45401188 95.07143064 73.19939418]\n",
            " [59.86584842 15.60186404 15.59945203]\n",
            " [ 5.80836122 86.61761458 60.11150117]\n",
            " [70.80725778  2.05844943 96.99098522]]\n",
            "\n",
            "A_1 (4 x 3):\n",
            "[[83.24426408 21.23391107 18.18249672]\n",
            " [18.34045099 30.4242243  52.47564316]\n",
            " [43.19450186 29.12291402 61.18528947]\n",
            " [13.94938607 29.21446485 36.63618433]]\n",
            "\n",
            "A_2 (4 x 3):\n",
            "[[45.60699842 78.51759614 19.96737822]\n",
            " [51.42344384 59.24145689  4.64504127]\n",
            " [60.75448519 17.05241237  6.5051593 ]\n",
            " [94.88855373 96.56320331 80.83973481]]\n",
            "\n",
            "A_3 (4 x 3):\n",
            "[[30.46137692  9.7672114  68.42330265]\n",
            " [44.01524937 12.20382348 49.51769101]\n",
            " [ 3.43885211 90.93204021 25.87799816]\n",
            " [66.25222844 31.17110761 52.00680212]]\n",
            "\n",
            "R_00 (3 x 3):\n",
            "[[-100.1704928   -51.34930187 -108.73761435]\n",
            " [   0.          118.96256828   59.05485274]\n",
            " [   0.            0.           57.53949594]]\n",
            "\n",
            "R_01 (3 x 3):\n",
            "[[ 139.14186155   65.65065092  118.74382109]\n",
            " [   0.         -124.72761109  -77.35356272]\n",
            " [   0.            0.          -81.90613682]]\n",
            "\n",
            "R_02 (3 x 3):\n",
            "[[-191.78052981 -135.36754049 -134.2041117 ]\n",
            " [   0.          144.33438272   62.61083117]\n",
            " [   0.            0.          108.89938755]]\n",
            "\n",
            "R_03 (3 x 3):\n",
            "[[ 209.87184841  139.00574603  159.79320034]\n",
            " [   0.         -171.22731452  -59.79456032]\n",
            " [   0.            0.         -123.24288373]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Reconstructing A\n",
        "# ============================================================\n",
        "# Build embedded m x m orthogonal matrices\n",
        "# Each M_i embeds Q_{0i} into an m x m identity matrix.\n",
        "def embed_Q(Q, indices, m):\n",
        "    \"\"\"Embed square matrix Q into m x m identity at the given row/col indices.\"\"\"\n",
        "    M = np.eye(m)\n",
        "    for a, row_a in enumerate(indices):\n",
        "        for b, row_b in enumerate(indices):\n",
        "            M[row_a, row_b] = Q[a, b]\n",
        "    return M\n",
        "\n",
        "Ms = []\n",
        "for i in range(p):\n",
        "    if i == 0:\n",
        "        indices = list(range(mb))\n",
        "    else:\n",
        "        indices = list(range(n)) + list(range(i * mb, (i + 1) * mb))\n",
        "    print(f\"M{i+1} embeds Q_0{i} ({Qs_full[i].shape}) at rows {indices[:3]}...{indices[-3:]}\")\n",
        "    Ms.append(embed_Q(Qs_full[i], indices, m))\n",
        "\n",
        "# Embed R_final into m x n  (zeros below)\n",
        "R_embedded = np.zeros((m, n))\n",
        "R_embedded[:n, :] = R_final\n",
        "\n",
        "# Reconstruct A = M1 @ M2 @ M3 @ M4 @ R_embedded\n",
        "computed_A = Ms[0] @ Ms[1] @ Ms[2] @ Ms[3] @ R_embedded\n",
        "\n",
        "print(f\"\\nMax reconstruction error: {np.max(np.abs(computed_A - A)):.4e}\\n\")\n",
        "print(f\"A:\\n{np.round(A, 4)}\\n\")\n",
        "print(f\"computed_A:\\n{np.round(computed_A, 4)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAO9_US2fx_z",
        "outputId": "be080ed7-85ff-45c6-c1e0-ed12cf83bd0a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M1 embeds Q_00 ((4, 4)) at rows [0, 1, 2]...[1, 2, 3]\n",
            "M2 embeds Q_01 ((7, 7)) at rows [0, 1, 2]...[5, 6, 7]\n",
            "M3 embeds Q_02 ((7, 7)) at rows [0, 1, 2]...[9, 10, 11]\n",
            "M4 embeds Q_03 ((7, 7)) at rows [0, 1, 2]...[13, 14, 15]\n",
            "\n",
            "Max reconstruction error: 4.2633e-14\n",
            "\n",
            "A:\n",
            "[[37.454  95.0714 73.1994]\n",
            " [59.8658 15.6019 15.5995]\n",
            " [ 5.8084 86.6176 60.1115]\n",
            " [70.8073  2.0584 96.991 ]\n",
            " [83.2443 21.2339 18.1825]\n",
            " [18.3405 30.4242 52.4756]\n",
            " [43.1945 29.1229 61.1853]\n",
            " [13.9494 29.2145 36.6362]\n",
            " [45.607  78.5176 19.9674]\n",
            " [51.4234 59.2415  4.645 ]\n",
            " [60.7545 17.0524  6.5052]\n",
            " [94.8886 96.5632 80.8397]\n",
            " [30.4614  9.7672 68.4233]\n",
            " [44.0152 12.2038 49.5177]\n",
            " [ 3.4389 90.932  25.878 ]\n",
            " [66.2522 31.1711 52.0068]]\n",
            "\n",
            "computed_A:\n",
            "[[37.454  95.0714 73.1994]\n",
            " [59.8658 15.6019 15.5995]\n",
            " [ 5.8084 86.6176 60.1115]\n",
            " [70.8073  2.0584 96.991 ]\n",
            " [83.2443 21.2339 18.1825]\n",
            " [18.3405 30.4242 52.4756]\n",
            " [43.1945 29.1229 61.1853]\n",
            " [13.9494 29.2145 36.6362]\n",
            " [45.607  78.5176 19.9674]\n",
            " [51.4234 59.2415  4.645 ]\n",
            " [60.7545 17.0524  6.5052]\n",
            " [94.8886 96.5632 80.8397]\n",
            " [30.4614  9.7672 68.4233]\n",
            " [44.0152 12.2038 49.5177]\n",
            " [ 3.4389 90.932  25.878 ]\n",
            " [66.2522 31.1711 52.0068]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential TSQR with Thin QR (mode='reduced')\n",
        "Qs_thin   = [None] * p\n",
        "R_current = None\n",
        "\n",
        "for i in range(p):\n",
        "    if i == 0:\n",
        "        Qs_thin[i], R_current = np.linalg.qr(A_blocks[i], mode='reduced')\n",
        "        print(f\"R_0{i} ({R_current.shape[0]} x {R_current.shape[1]}):\\n{R_current}\\n\")\n",
        "    else:\n",
        "        stacked = np.vstack((R_current, A_blocks[i]))\n",
        "        Qs_thin[i], R_current = np.linalg.qr(stacked, mode='reduced')\n",
        "        print(f\"R_0{i} ({R_current.shape[0]} x {R_current.shape[1]}):\\n{R_current}\\n\")\n",
        "\n",
        "R_final = R_current  # n × n\n",
        "\n",
        "# Reconstructing A\n",
        "# Unroll the chain (no embedding needed)\n",
        "R2 = Qs_thin[3][:n, :] @ R_final\n",
        "R1 = Qs_thin[2][:n, :] @ R2\n",
        "R0 = Qs_thin[1][:n, :] @ R1\n",
        "\n",
        "A0_rec = Qs_thin[0]        @ R0\n",
        "A1_rec = Qs_thin[1][n:, :] @ R1\n",
        "A2_rec = Qs_thin[2][n:, :] @ R2\n",
        "A3_rec = Qs_thin[3][n:, :] @ R_final\n",
        "\n",
        "computed_A = np.vstack([A0_rec, A1_rec, A2_rec, A3_rec])\n",
        "print(f\"Max reconstruction error: {np.max(np.abs(computed_A - A)):.2e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4T9WMrxibPx",
        "outputId": "f4477b85-0945-45b7-e74c-0094a3b8b9d7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R_00 (3 x 3):\n",
            "[[-100.1704928   -51.34930187 -108.73761435]\n",
            " [   0.          118.96256828   59.05485274]\n",
            " [   0.            0.           57.53949594]]\n",
            "\n",
            "R_01 (3 x 3):\n",
            "[[ 139.14186155   65.65065092  118.74382109]\n",
            " [   0.         -124.72761109  -77.35356272]\n",
            " [   0.            0.          -81.90613682]]\n",
            "\n",
            "R_02 (3 x 3):\n",
            "[[-191.78052981 -135.36754049 -134.2041117 ]\n",
            " [   0.          144.33438272   62.61083117]\n",
            " [   0.            0.          108.89938755]]\n",
            "\n",
            "R_03 (3 x 3):\n",
            "[[ 209.87184841  139.00574603  159.79320034]\n",
            " [   0.         -171.22731452  -59.79456032]\n",
            " [   0.            0.         -123.24288373]]\n",
            "\n",
            "Max reconstruction error: 5.68e-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "- [1] Jammes Demmel, Laura Grigori, Mark Hoemmen, and Julien Langou. Implementing Communication-Optimal Parallel And Sequential QR Factorizations. https://arxiv.abs/pdf/0809.2407, 2008. arXiv:0809.2407 [math.NA]"
      ],
      "metadata": {
        "id": "Ol13ebYfUaWA"
      }
    }
  ]
}